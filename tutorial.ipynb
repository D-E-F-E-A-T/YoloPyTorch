{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "YoloPyTorch - ultralytics",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HvhYZrIZCEyo"
      },
      "source": [
        "\n",
        "<table align=\"center\"><td>\n",
        "  <a target=\"_blank\"  href=\"https://github.com/ultralytics/yolov3/blob/master/tutorial.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on github\n",
        "  </a>\n",
        "</td><td>\n",
        "  <a target=\"_blank\"  href=\"https://colab.sandbox.google.com/github/ultralytics/yolov3/blob/master/tutorial.ipynb\">\n",
        "    <img width=32px src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "</td></table>\n",
        "\n",
        "This notebook contains software developed by Ultralytics LLC, and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://github.com/ultralytics/yolov3 and https://www.ultralytics.com.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e5ylFIvlCEym",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import glob\n",
        "import torch\n",
        "import os\n",
        "\n",
        "from IPython.display import Image, clear_output \n",
        "print('PyTorch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLUfFCXZzREx",
        "colab_type": "text"
      },
      "source": [
        "Install NVIDIA Apex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4L1OteSzL06",
        "colab_type": "code",
        "outputId": "f8d940f9-3db7-4f52-b36d-cb8de8cf83b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%%writefile setup.sh\n",
        "\n",
        "export CUDA_HOME=/usr/local/cuda-10.1\n",
        "git clone https://github.com/NVIDIA/apex\n",
        "pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting setup.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dqbdsfp4z5v_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sh setup.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTyaTagv5PWw",
        "colab_type": "text"
      },
      "source": [
        "Load TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGYKIiZN5vPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf /content/YoloPyTorch/runs/*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij7UXLz75OFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/YoloPyTorch/runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52YiNHJp0I7S",
        "colab_type": "text"
      },
      "source": [
        "Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOoc6Law6kIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# /content/drive/My Drive/Jewelry/images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "---\n",
        "\n",
        "*Clone* repository and download COCO 2014 dataset (20GB):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tIFv0p1TCEyj",
        "colab": {}
      },
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/Goldenstarc/YoloPyTorch.git  # clone\n",
        "# !git clone https://github.com/ultralytics/yolov3  # clone\n",
        "!bash YoloPyTorch/data/get_coco2017.sh  # copy COCO2017 dataset (19GB)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ijTFlKcp6JVy"
      },
      "source": [
        "Run `train.py` to train YOLOv3-Tiny starting from a darknet53 backbone:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mupsoa0lzSPo",
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "ec54f8e0-cc08-4313-ae1e-cfb0f5c65a94"
      },
      "source": [
        "%cd /content/YoloPyTorch\n",
        "# !python3 train.py --cfg yolov3-spp-1cls.cfg --data data/jewelry1cls.data --weights '' --name ring1 --nosave --single-cls --cache-images --batch-size 8\n",
        "!python3 train.py --cfg yolov3-tiny-7cls.cfg --data data/jewelry.data --weights '' --name ringAll --nosave --cache-images --batch-size 16"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/YoloPyTorch\n",
            "Namespace(adam=False, batch_size=16, bucket='', cache_images=True, cfg='yolov3-tiny-7cls.cfg', data='data/jewelry.data', device='', epochs=300, evolve=False, img_size=[320, 640], multi_scale=False, name='ringAll', nosave=True, notest=False, rect=False, resume=False, single_cls=False, weights='')\n",
            "Using CUDA Apex device0 _CudaDeviceProperties(name='Tesla K80', total_memory=11441MB)\n",
            "\n",
            "Start Tensorboard with \"tensorboard --logdir=runs\", view at http://localhost:6006/\n",
            "2020-05-07 20:49:23.365544: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "Model Summary: 37 layers, 8.68374e+06 parameters, 8.68374e+06 gradients\n",
            "Optimizer groups: 13 .bias, 13 Conv2d.weight, 11 other\n",
            "Caching labels (806 found, 0 missing, 0 empty, 0 duplicate, for 806 images): 100% 806/806 [00:00<00:00, 6610.28it/s]\n",
            "Caching images (1.0GB): 100% 806/806 [00:08<00:00, 96.13it/s]\n",
            "Caching labels (806 found, 0 missing, 0 empty, 0 duplicate, for 806 images): 100% 806/806 [00:00<00:00, 6514.86it/s]\n",
            "Caching images (0.9GB):  89% 720/806 [00:11<00:01, 64.00it/s]Traceback (most recent call last):\n",
            "  File \"train.py\", line 412, in <module>\n",
            "    train()  # train normally\n",
            "  File \"train.py\", line 200, in train\n",
            "    single_cls=opt.single_cls),\n",
            "  File \"/content/YoloPyTorch/utils/datasets.py\", line 388, in __init__\n",
            "    self.imgs[i], self.img_hw0[i], self.img_hw[i] = load_image(self, i)  # img, hw_original, hw_resized\n",
            "  File \"/content/YoloPyTorch/utils/datasets.py\", line 510, in load_image\n",
            "    img = cv2.resize(img, (int(w0 * r), int(h0 * r)), interpolation=interp)\n",
            "KeyboardInterrupt\n",
            "Caching images (0.9GB):  89% 720/806 [00:11<00:01, 62.24it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0eq1SMWl6Sfn"
      },
      "source": [
        "Run `test.py` to evaluate the performance of a trained darknet or PyTorch model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0v0RFtO-WG9o",
        "colab": {}
      },
      "source": [
        "%cd /content/YoloPyTorch/\n",
        "\n",
        "!python3 test.py --cfg yolov3-tiny-7cls.cfg --data data/jewelry.data  --weights weights/last_ringAll.pt --img 640 --augment --save-json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N3qM6T0W53gh"
      },
      "source": [
        "Run `detect.py` to perform inference on images in `data/samples` folder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zR9ZbuQCH7FX",
        "colab": {}
      },
      "source": [
        "!python3 detect.py --cfg yolov3-tiny-7cls.cfg --weights weights/last_ringAll.pt --name data/jewelry.names --source 0\n",
        "Image(filename='output/zidane.jpg', width=600)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VUOiNLtMP5aG"
      },
      "source": [
        "Reproduce tutorial training runs and plot training results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LA9qqd_NCEyB",
        "colab": {}
      },
      "source": [
        "!python3 train.py --data data/coco_16img.data --batch-size 16 --accumulate 1 --nosave && mv results.txt results_coco_16img.txt  # CUSTOM TRAINING EXAMPLE\n",
        "!python3 train.py --data data/coco_64img.data --batch-size 16 --accumulate 1 --nosave && mv results.txt results_coco_64img.txt \n",
        "!python3 -c \"from utils import utils; utils.plot_results()\"  # plot training results\n",
        "Image(filename='results.png', width=800)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "14mT7T7Q6erR"
      },
      "source": [
        "Extras below\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "42_zEpW6W_N1",
        "outputId": "03c422a7-0f62-4ba8-c159-0467ca06f8d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "!git pull"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects:  16% (1/6)\u001b[K\rremote: Counting objects:  33% (2/6)\u001b[K\rremote: Counting objects:  50% (3/6)\u001b[K\rremote: Counting objects:  66% (4/6)\u001b[K\rremote: Counting objects:  83% (5/6)\u001b[K\rremote: Counting objects: 100% (6/6)\u001b[K\rremote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects:  50% (1/2)\u001b[K\rremote: Compressing objects: 100% (2/2)\u001b[K\rremote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 4 (delta 2), reused 4 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects:  25% (1/4)   \rUnpacking objects:  50% (2/4)   \rUnpacking objects:  75% (3/4)   \rUnpacking objects: 100% (4/4)   \rUnpacking objects: 100% (4/4), done.\n",
            "From https://github.com/Goldenstarc/YoloPyTorch\n",
            "   0188cad..5d80078  master     -> origin/master\n",
            "Updating 0188cad..5d80078\n",
            "Fast-forward\n",
            " cfg/yolov3-tiny-7cls.cfg | 182 \u001b[32m+++++++++++++++++++++++++++++++++++++++++++++++\u001b[m\n",
            " 1 file changed, 182 insertions(+)\n",
            " create mode 100644 cfg/yolov3-tiny-7cls.cfg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uB3v5hj_CEyI",
        "colab": {}
      },
      "source": [
        "# Unit Tests\n",
        "!python3 detect.py --cfg yolov3-tiny-7cls.cfg  --weights weights/last_ringAll.pt --name data/jewelry.names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6D0si0TNCEx5",
        "colab": {}
      },
      "source": [
        "# Evolve Hyperparameters\n",
        "!python3 train.py --data data/coco.data --img-size 320 --epochs 1 --evolve"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}